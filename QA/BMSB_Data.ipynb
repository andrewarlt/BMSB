{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMSB Observation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Download MN Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Minnesota Boundary from: https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_dnr/bdry_state_of_minnesota/fgdb_bdry_state_of_minnesota.zip\n",
      "Downloaded Minnesota Boundary: C:\\Users\\ethan\\Desktop\\GIS5572\\MN_Boundary\\mn_boundary.zip\n",
      "Extracted Minnesota Boundary to: C:\\Users\\ethan\\Desktop\\GIS5572\\MN_Boundary\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "# Base URL for Minnesota GIS data\n",
    "MN_BOUNDARY_URL = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_dnr/bdry_state_of_minnesota/fgdb_bdry_state_of_minnesota.zip\"\n",
    "FOLDER_PATH = \"GIS5572\"\n",
    "DESKTOP_PATH = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "DATASET_DIRECTORY = os.path.join(DESKTOP_PATH, FOLDER_PATH)\n",
    "\n",
    "# Function to Retrieve MN Boundary\n",
    "def download_mn_boundary():\n",
    "    SAVE_DIR = os.path.join(DATASET_DIRECTORY, \"MN_Boundary\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    file_name = os.path.join(SAVE_DIR, \"mn_boundary.zip\")\n",
    "\n",
    "    print(f\"Downloading Minnesota Boundary from: {MN_BOUNDARY_URL}\")\n",
    "\n",
    "    # Download the shapefile zip\n",
    "    with requests.get(MN_BOUNDARY_URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(f\"Downloaded Minnesota Boundary: {file_name}\")\n",
    "\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(SAVE_DIR)\n",
    "    print(f\"Extracted Minnesota Boundary to: {SAVE_DIR}\")\n",
    "\n",
    "# Call function to download the MN boundary shapefile\n",
    "download_mn_boundary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Retrieve BSMB Observation Data from iNaturalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iNaturalist API base URL\n",
    "BASE_URL = \"https://api.inaturalist.org/v1/observations\"\n",
    "\n",
    "# Query parameters (Use bounding box for Minnesota)\n",
    "params = {\n",
    "    \"taxon_id\": 81923,      # Brown Marmorated Stink Bug\n",
    "    \"verifiable\": \"true\",   # Only verifiable observations\n",
    "    \"per_page\": 200,        # Max 200 results per request (iNaturalist's max)\n",
    "    \"swlat\": 43.499,        # Southwest latitude (MN)\n",
    "    \"swlng\": -97.2392,      # Southwest longitude (MN)\n",
    "    \"nelat\": 49.3843,       # Northeast latitude (MN)\n",
    "    \"nelng\": -89.4917       # Northeast longitude (MN)\n",
    "}\n",
    "\n",
    "# Initialize a list to store all observations\n",
    "all_observations = []\n",
    "page = 1  # Start with the first page\n",
    "\n",
    "while True:\n",
    "    # Update the page number in the parameters\n",
    "    params[\"page\"] = page\n",
    "    \n",
    "    # Send GET request to iNaturalist API\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if data retrieval was successful\n",
    "    if \"results\" in data:\n",
    "        # If no more results, stop the loop\n",
    "        if not data[\"results\"]:\n",
    "            break\n",
    "        \n",
    "        # Process the observations\n",
    "        for obs in data[\"results\"]:\n",
    "            lat = obs.get(\"geojson\", {}).get(\"coordinates\", [None, None])[1]\n",
    "            lon = obs.get(\"geojson\", {}).get(\"coordinates\", [None, None])[0]\n",
    "\n",
    "            all_observations.append({\n",
    "                \"ID\": obs[\"id\"],\n",
    "                \"Observed Date\": obs.get(\"observed_on\", \"N/A\"),\n",
    "                \"Latitude\": lat,\n",
    "                \"Longitude\": lon,\n",
    "                \"Scientific Name\": obs[\"taxon\"][\"name\"] if \"taxon\" in obs else \"N/A\",\n",
    "                \"Common Name\": obs[\"taxon\"].get(\"preferred_common_name\", \"N/A\"),\n",
    "                \"Image URL\": obs[\"photos\"][0][\"url\"] if obs.get(\"photos\") else None,\n",
    "                \"Location Description\": obs.get(\"place_guess\", \"Unknown\"),\n",
    "                \"Exact Location?\": obs.get(\"location_is_exact\", False),\n",
    "                \"Obscured?\": obs.get(\"obscured\", False),\n",
    "                \"Accuracy (meters)\": obs.get(\"positional_accuracy\", \"N/A\"),\n",
    "                \"Count Observed\": obs.get(\"individual_count\", \"N/A\")\n",
    "            })\n",
    "        \n",
    "        # Increment the page number for the next request\n",
    "        page += 1\n",
    "    else:\n",
    "        print(\"Failed to fetch observation data.\")\n",
    "        break\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(all_observations)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"brown_marmorated_stink_bug_mn_all_observations.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} observations to {csv_filename}\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Retrieve MN BMSB Survey Data, Modify for Presence/Absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_mda/biota_bmsb/shp_biota_bmsb.zip\n",
      "Downloaded: C:\\Users\\ethan\\Desktop\\GIS5572\\biota-bmsb\\shp_biota_bmsb.zip\n",
      "Extracted: C:\\Users\\ethan\\Desktop\\GIS5572\\biota-bmsb\n",
      "Downloading: https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_mda/biota_bmsb/fgdb_biota_bmsb.zip\n",
      "Downloaded: C:\\Users\\ethan\\Desktop\\GIS5572\\biota-bmsb\\fgdb_biota_bmsb.zip\n",
      "Extracted: C:\\Users\\ethan\\Desktop\\GIS5572\\biota-bmsb\n",
      "Downloading: https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_mda/biota_bmsb/gpkg_biota_bmsb.zip\n",
      "Downloaded: C:\\Users\\ethan\\Desktop\\GIS5572\\biota-bmsb\\gpkg_biota_bmsb.zip\n",
      "Extracted: C:\\Users\\ethan\\Desktop\\GIS5572\\biota-bmsb\n",
      "Failed to fetch dataset information.\n"
     ]
    },
    {
     "ename": "<class 'FileExistsError'>",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\ethan\\\\Desktop\\\\GIS5572\\\\OUTPUT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# EXPORT FEATURE CLASS TO CSV\u001b[39;00m\n\u001b[0;32m    138\u001b[0m BMSB_OUTPUT_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_DIRECTORY, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbmsb_mndoag.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_DIRECTORY, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    140\u001b[0m arcpy\u001b[38;5;241m.\u001b[39mconversion\u001b[38;5;241m.\u001b[39mExportTable(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBMSBSurveyData_PairwiseClip\u001b[39m\u001b[38;5;124m\"\u001b[39m, BMSB_OUTPUT_PATH)\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\ethan\\\\Desktop\\\\GIS5572\\\\OUTPUT'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "import requests\n",
    "import arcpy\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Base URL for Minnesota GIS data\n",
    "BASE_URL = \"https://gisdata.mn.gov/api/3/action/\"\n",
    "\n",
    "FOLDER_PATH = \"GIS5572\"\n",
    "DESKTOP_PATH = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "DATASET_DIRECTORY = os.path.join(DESKTOP_PATH, FOLDER_PATH)\n",
    "MN_BOUNDARY = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\MN_Boundary\\bdry_state_of_minnesota.gdb\\state_of_minnesota\"\n",
    "\n",
    "# FUNCTION TO PULL DATA FROM MNGIS STORE IT IN THE HOME FOLDER\n",
    "def pullFromMNGIS(datasetName: str):\n",
    "    SAVE_DIR = os.path.join(DESKTOP_PATH, FOLDER_PATH, datasetName)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # Fetch dataset details\n",
    "    response = requests.get(f\"{BASE_URL}package_show\", params={\"id\": datasetName})\n",
    "    data = response.json()\n",
    "\n",
    "    if data[\"success\"]:\n",
    "        resources = data[\"result\"][\"resources\"]\n",
    "\n",
    "        # Find the TIFF or ZIP resource\n",
    "        tiff_url = None\n",
    "        for resource in resources:\n",
    "            if \"tif\" in resource[\"url\"].lower() or \"zip\" in resource[\"url\"].lower():\n",
    "                tiff_url = resource[\"url\"]\n",
    "                file_name = os.path.join(SAVE_DIR, os.path.basename(tiff_url))\n",
    "                print(f\"Downloading: {tiff_url}\")\n",
    "\n",
    "                # Download the file\n",
    "                with requests.get(tiff_url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(file_name, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "                # Unzip if it's a ZIP file\n",
    "                if file_name.endswith(\".zip\"):\n",
    "                    with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
    "                        zip_ref.extractall(SAVE_DIR)\n",
    "                    print(f\"Extracted: {SAVE_DIR}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to fetch dataset information.\")\n",
    "\n",
    "pullFromMNGIS(\"biota-bmsb\")\n",
    "\n",
    "# Path to your GeoPackage DATASET\n",
    "BMSB_DATASET_PATH = os.path.join(DATASET_DIRECTORY, \"biota-bmsb\")\n",
    "gpkg_path = os.path.join(BMSB_DATASET_PATH, \"BMSBSurveyDataTable.dbf\")\n",
    "\n",
    "BMSB_OUTPUT_PATH = os.path.join(DATASET_DIRECTORY, \"OUTPUT\", \"bmsb_mndoag.csv\")\n",
    "\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map_obj = aprx.listMaps()[0]  # Assuming you want to add to the first map\n",
    "\n",
    "map_obj.addDataFromPath(gpkg_path)\n",
    "\n",
    "# CONVERT TO XY POINTS\n",
    "arcpy.management.XYTableToPoint(\n",
    "    in_table=\"BMSBSurveyDataTable\",\n",
    "    out_feature_class=r\"BMSBSurveyData_XY_Points\",\n",
    "    x_field=\"Longitude\",\n",
    "    y_field=\"Latitude\",\n",
    "    z_field=None,\n",
    "    coordinate_system='GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]];-400 -400 1000000000;-100000 10000;-100000 10000;8.98315284119521E-09;0.001;0.001;IsHighPrecision'\n",
    ")\n",
    "\n",
    "# ADDS CTU DATA TO XY POINTS\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=\"BMSBSurveyData_XY_Points\",\n",
    "    join_features=\"ctus_with_id\",\n",
    "    out_feature_class=r\"BMSBSurveyData_SpatialJoin\",\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_ALL\",\n",
    "    field_mapping='SiteName \"SiteName\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,SiteName,0,254;SiteType \"SiteType\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,SiteType,0,254;City \"City\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,City,0,254;County \"County\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,County,0,254;Latitude \"Latitude\" true true false 8 Double 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Latitude,-1,-1;Longitude \"Longitude\" true true false 8 Double 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Longitude,-1,-1;SurveyName \"SurveyName\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,SurveyName,0,254;Scientific \"Scientific\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Scientific,0,254;CommonName \"CommonName\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,CommonName,0,254;Surveyor \"Surveyor\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Surveyor,0,254;Year \"Year\" true true false 8 Double 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Year,-1,-1;TrapID \"TrapID\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,TrapID,0,254;TrapType \"TrapType\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,TrapType,0,254;Lure \"Lure\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Lure,0,254;CheckDate \"CheckDate\" true true false 8 Date 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,CheckDate,-1,-1;Adults \"Adults\" true true false 8 Double 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Adults,-1,-1;Nymphs \"Nymphs\" true true false 8 Double 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Nymphs,-1,-1;Notes \"Notes\" true true false 254 Text 0 0,First,#,BMSBSurveyDataTable_XYTableToPoint1,Notes,0,254;GNIS_FEATU \"GNIS_FEATU\" true true false 10 Long 0 10,First,#,ctus_with_id,GNIS_FEATU,-1,-1;FEATURE_NA \"FEATURE_NA\" true true false 254 Text 0 0,First,#,ctus_with_id,FEATURE_NA,0,254;CTU_CLASS \"CTU_CLASS\" true true false 25 Text 0 0,First,#,ctus_with_id,CTU_CLASS,0,25;COUNTY_GNI \"COUNTY_GNI\" true true false 10 Long 0 10,First,#,ctus_with_id,COUNTY_GNI,-1,-1;COUNTY_COD \"COUNTY_COD\" true true false 2 Text 0 0,First,#,ctus_with_id,COUNTY_COD,0,2;COUNTY_NAM \"COUNTY_NAM\" true true false 100 Text 0 0,First,#,ctus_with_id,COUNTY_NAM,0,100;POPULATION \"POPULATION\" true true false 10 Long 0 10,First,#,ctus_with_id,POPULATION,-1,-1;SHAPE_Leng \"SHAPE_Leng\" true true false 19 Double 0 0,First,#,ctus_with_id,SHAPE_Leng,-1,-1;SHAPE_Area \"SHAPE_Area\" true true false 19 Double 0 0,First,#,ctus_with_id,SHAPE_Area,-1,-1;ORIG_FID \"ORIG_FID\" true true false 10 Long 0 10,First,#,ctus_with_id,ORIG_FID,-1,-1;UNIQUE_ID \"UNIQUE_ID\" true true false 10 Long 0 10,First,#,ctus_with_id,UNIQUE_ID,-1,-1',\n",
    "    match_option=\"WITHIN\",\n",
    "    search_radius=None,\n",
    "    distance_field_name=\"\"\n",
    ")\n",
    "\n",
    "# CLIPS OUT ANYTHING OUTSIDE OF MN BOUNDARIES\n",
    "arcpy.analysis.PairwiseClip(\n",
    "    in_features=\"BMSBSurveyData_SpatialJoin\",\n",
    "    clip_features= MN_BOUNDARY,\n",
    "    out_feature_class=r\"BMSBSurveyData_PairwiseClip\",\n",
    "    cluster_tolerance=None\n",
    ")\n",
    "\n",
    "# ADDS FIELD FOR PRESENCE OF BMSB\n",
    "arcpy.management.AddField(\n",
    "    in_table=\"BMSBSurveyData_PairwiseClip\",\n",
    "    field_name=\"Pres_Abs\",\n",
    "    field_type=\"SHORT\",\n",
    "    field_precision=None,\n",
    "    field_scale=None,\n",
    "    field_length=None,\n",
    "    field_alias=\"\",\n",
    "    field_is_nullable=\"NULLABLE\",\n",
    "    field_is_required=\"NON_REQUIRED\",\n",
    "    field_domain=\"\"\n",
    ")\n",
    "\n",
    "# ADDS FIELD TO PUT IN MONTH\n",
    "arcpy.management.AddField(\n",
    "    in_table=\"BMSBSurveyData_PairwiseClip\",\n",
    "    field_name=\"Month\",\n",
    "    field_type=\"SHORT\",\n",
    "    field_precision=None,\n",
    "    field_scale=None,\n",
    "    field_length=None,\n",
    "    field_alias=\"\",\n",
    "    field_is_nullable=\"NULLABLE\",\n",
    "    field_is_required=\"NON_REQUIRED\",\n",
    "    field_domain=\"\"\n",
    ")\n",
    "\n",
    "# Iterate through and clean rows\n",
    "with arcpy.da.UpdateCursor(r\"BMSBSurveyData_PairwiseClip\", ['Year', 'SiteType', 'Latitude', 'Longitude', 'Adults', 'Nymphs', 'Notes', 'CheckDate','Pres_Abs','Month']) as cursor:  # Use OBJECTID or another unique identifier\n",
    "    # Iterate through the rows\n",
    "    for row in cursor:\n",
    "        # Replace this condition with your logic to identify rows to delete\n",
    "        if ( not ((row[0] <= 2025) and (row[0] >= 2000) and (row[4] >= 0) and (row[4] <= 1000000) and (row[5] >= 0) and (row[5] <= 1000000) and (row[6] != \"\"\"Gate Locked\"\"\") and (row[6] != \"\"\"Gate Locked - didn't check\"\"\"))):\n",
    "            # BAD ROW. QAQC BAD DATA REMOVAL\n",
    "            cursor.deleteRow()   \n",
    "        else:\n",
    "            # SETS MONTH AND BMSB PRESENCE\n",
    "            cursor.updateRow(row[:8]+[int((row[4]+row[5])>0),row[7].month])\n",
    "\n",
    "# EXPORT FEATURE CLASS TO CSV\n",
    "BMSB_OUTPUT_PATH = os.path.join(DATASET_DIRECTORY, \"OUTPUT\", \"bmsb_mndoag.csv\")\n",
    "os.mkdir(os.path.join(DATASET_DIRECTORY, \"OUTPUT\"))\n",
    "arcpy.conversion.ExportTable(r\"BMSBSurveyData_PairwiseClip\", BMSB_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Retrieve iNaturalist BMSB Observations, Modify for Presence/Absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading iNaturalist data...\n",
      "Retrieved page 1 with 200 records.\n",
      "Retrieved page 2 with 200 records.\n",
      "Retrieved page 3 with 200 records.\n",
      "Retrieved page 4 with 200 records.\n",
      "Retrieved page 5 with 200 records.\n",
      "Retrieved page 6 with 200 records.\n",
      "Retrieved page 7 with 95 records.\n",
      "Downloaded 904 total observations.\n",
      "Saved observations to CSV at: C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_inat_test.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import urllib3\n",
    "import arcpy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "API_TOKEN = \"YOUR_API_TOKEN_HERE\"  # Replace with your real token\n",
    "BASE_URL = \"https://api.inaturalist.org/v1/observations\"\n",
    "\n",
    "FOLDER_NAME = \"GIS5572\"\n",
    "DESKTOP_PATH = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "DATASET_DIR = os.path.join(DESKTOP_PATH, FOLDER_NAME)\n",
    "OUTPUT_DIR = os.path.join(DATASET_DIR, \"OUTPUT\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CSV_PATH = os.path.join(OUTPUT_DIR, \"bmsb_inat_test.csv\")\n",
    "FINAL_OUTPUT = os.path.join(OUTPUT_DIR, \"bmsb_inat_final.shp\")\n",
    "\n",
    "XY_FEATURE = os.path.join(\"memory\", \"BMSB_inat_XY\")\n",
    "SPATIAL_JOIN_OUTPUT = os.path.join(\"memory\", \"BMSB_inat_SpatialJoin\")\n",
    "CLIPPED_OUTPUT = os.path.join(\"memory\", \"BMSB_inat_Clip\")\n",
    "\n",
    "MN_BOUNDARY = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\MN_Boundary\\bdry_state_of_minnesota.gdb\\state_of_minnesota\"\n",
    "CTU_LAYER = \"ctus_with_id\"  # This should be available in the current map or workspace\n",
    "\n",
    "\n",
    "# --- Step 1: Pull data from iNaturalist ---\n",
    "def pull_inaturalist_data():\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    params = {\n",
    "        \"taxon_id\": 81923,  # BMSB\n",
    "        \"verifiable\": \"true\",\n",
    "        \"per_page\": 200,\n",
    "        \"swlat\": 43.499,\n",
    "        \"swlng\": -97.2392,\n",
    "        \"nelat\": 49.3843,\n",
    "        \"nelng\": -89.4917\n",
    "    }\n",
    "\n",
    "    page = 1\n",
    "    all_obs = []\n",
    "    print(\"Downloading iNaturalist data...\")\n",
    "\n",
    "    while True:\n",
    "        params[\"page\"] = page\n",
    "        r = requests.get(BASE_URL, params=params, headers=headers, verify=False)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Error {r.status_code}: {r.text}\")\n",
    "            break\n",
    "\n",
    "        results = r.json().get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for obs in results:\n",
    "            if obs.get(\"species_guess\") == \"Brown Marmorated Stink Bug\":\n",
    "                geojson = obs.get(\"geojson\")\n",
    "                if geojson:\n",
    "                    coordinates = geojson.get(\"coordinates\", [])\n",
    "                    if coordinates:\n",
    "                        date_str = obs.get(\"observed_on\")\n",
    "                        try:\n",
    "                            parsed_date = datetime.strptime(date_str, \"%Y-%m-%d\") if date_str else None\n",
    "                            month = parsed_date.month if parsed_date else None\n",
    "                            year = parsed_date.year if parsed_date else None\n",
    "                        except Exception:\n",
    "                            parsed_date = None\n",
    "                            month = None\n",
    "                            year = None\n",
    "\n",
    "                        all_obs.append({\n",
    "                            \"Observation ID\": obs.get(\"id\"),\n",
    "                            \"Species\": obs.get(\"species_guess\"),\n",
    "                            \"Latitude\": coordinates[1],\n",
    "                            \"Longitude\": coordinates[0],\n",
    "                            \"Date\": date_str,\n",
    "                            \"Month\": month,\n",
    "                            \"Year\": year,\n",
    "                            \"Pres_Abs\": 1,\n",
    "                            \"GeoJSON\": json.dumps(geojson)\n",
    "                        })\n",
    "\n",
    "        print(f\"Retrieved page {page} with {len(results)} records.\")\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"Downloaded {len(all_obs)} total observations.\")\n",
    "    return all_obs\n",
    "\n",
    "\n",
    "# --- Step 2: Process and Save to CSV ---\n",
    "obs_list = pull_inaturalist_data()\n",
    "df = pd.DataFrame(obs_list)\n",
    "df[\"Latitude\"] = pd.to_numeric(df[\"Latitude\"], errors='coerce')\n",
    "df[\"Longitude\"] = pd.to_numeric(df[\"Longitude\"], errors='coerce')\n",
    "df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"Saved observations to CSV at: {CSV_PATH}\")\n",
    "\n",
    "# --- Step 3: Convert Table to Points ---\n",
    "# Convert the Table to Points, for visual check of data\n",
    "coordinate_system = arcpy.SpatialReference(4326)\n",
    "\n",
    "arcpy.management.XYTableToPoint(\n",
    "    in_table=r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_inat_test.csv\",\n",
    "    out_feature_class=r\"C:\\Users\\ethan\\Documents\\ArcGIS\\Projects\\BMSB_Cleaning\\BMSB_Cleaning.gdb\\bmsb_inat_XYTableToPoint\",\n",
    "    x_field=\"Longitude\",\n",
    "    y_field=\"Latitude\",\n",
    "    coordinate_system=coordinate_system\n",
    ")\n",
    "\n",
    "print(\"CSV table successfully converted to point layer!\")\n",
    "\n",
    "# --- Step 4: Clip Points to MN Boundary ---\n",
    "arcpy.analysis.PairwiseClip(\n",
    "    in_features=\"bmsb_inat_XYTableToPoint\",\n",
    "    clip_features=r\"C:\\Users\\ethan\\Desktop\\GIS5572\\MN_Boundary\\bdry_state_of_minnesota.gdb\\state_of_minnesota\",\n",
    "    out_feature_class=r\"C:\\Users\\ethan\\Documents\\ArcGIS\\Projects\\BMSB_Cleaning\\BMSB_Cleaning.gdb\\bmsb_inat_PairwiseClip\",\n",
    "    cluster_tolerance=None\n",
    ")\n",
    "\n",
    "print(\"Point layer successfully clipped to MN Boundary!\")\n",
    "\n",
    "# --- Step 5: Spatially Join the Data with CTU Information ---\n",
    "\n",
    "with arcpy.EnvManager(outputCoordinateSystem='GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]]'):\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        target_features=\"ctus_with_id\",\n",
    "        join_features=\"bmsb_inat_PairwiseClip\",\n",
    "        out_feature_class=r\"C:\\Users\\ethan\\Documents\\ArcGIS\\Projects\\BMSB_Cleaning\\BMSB_Cleaning.gdb\\bmsb_inat_SpatialJoin\",\n",
    "        join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "        join_type=\"KEEP_ALL\",\n",
    "        field_mapping='GNIS_FEATU \"GNIS_FEATU\" true true false 10 Long 0 10,First,#,ctus_with_id,GNIS_FEATU,-1,-1;FEATURE_NA \"FEATURE_NA\" true true false 254 Text 0 0,First,#,ctus_with_id,FEATURE_NA,0,253;CTU_CLASS \"CTU_CLASS\" true true false 25 Text 0 0,First,#,ctus_with_id,CTU_CLASS,0,24;COUNTY_GNI \"COUNTY_GNI\" true true false 10 Long 0 10,First,#,ctus_with_id,COUNTY_GNI,-1,-1;COUNTY_COD \"COUNTY_COD\" true true false 2 Text 0 0,First,#,ctus_with_id,COUNTY_COD,0,1;COUNTY_NAM \"COUNTY_NAM\" true true false 100 Text 0 0,First,#,ctus_with_id,COUNTY_NAM,0,99;POPULATION \"POPULATION\" true true false 10 Long 0 10,First,#,ctus_with_id,POPULATION,-1,-1;SHAPE_Leng \"SHAPE_Leng\" true true false 19 Double 0 0,First,#,ctus_with_id,SHAPE_Leng,-1,-1;SHAPE_Area \"SHAPE_Area\" true true false 19 Double 0 0,First,#,ctus_with_id,SHAPE_Area,-1,-1;ORIG_FID \"ORIG_FID\" true true false 10 Long 0 10,First,#,ctus_with_id,ORIG_FID,-1,-1;UNIQUE_ID \"UNIQUE_ID\" true true false 10 Long 0 10,First,#,ctus_with_id,UNIQUE_ID,-1,-1;Observatio \"Observation ID\" true true false 4 Long 0 0,First,#,bmsb_inat_PairwiseClip,Observation_ID,-1,-1;Species \"Species\" true true false 8000 Text 0 0,First,#,bmsb_inat_PairwiseClip,Species,0,7999;Latitude \"Latitude\" true true false 8 Double 0 0,First,#,bmsb_inat_PairwiseClip,Latitude,-1,-1;Longitude \"Longitude\" true true false 8 Double 0 0,First,#,bmsb_inat_PairwiseClip,Longitude,-1,-1;Date \"Date\" true true false 8 DateOnly 0 0,First,#,bmsb_inat_PairwiseClip,Date,-1,-1;Month \"Month\" true true false 4 Long 0 0,First,#,bmsb_inat_PairwiseClip,Month,-1,-1;Year \"Year\" true true false 4 Long 0 0,First,#,bmsb_inat_PairwiseClip,Year,-1,-1;Pres_Abs \"Pres_Abs\" true true false 4 Long 0 0,First,#,bmsb_inat_PairwiseClip,Pres_Abs,-1,-1;GeoJSON \"GeoJSON\" true true false 8000 Text 0 0,First,#,bmsb_inat_PairwiseClip,GeoJSON,0,7999',\n",
    "        match_option=\"INTERSECT\",\n",
    "        search_radius=None,\n",
    "        distance_field_name=\"\",\n",
    "        match_fields=None\n",
    "    )\n",
    "\n",
    "print(\"Data was successfully joined!\")\n",
    "\n",
    "# --- Step 6: EXPORT FEATURE CLASS TO CSV ---\n",
    "BMSB_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"bmsb_inat_obs.csv\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "arcpy.conversion.ExportTable(\"bmsb_inat_SpatialJoin\", BMSB_OUTPUT_PATH)\n",
    "print(f\"Exported feature class to CSV: {BMSB_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported feature class to CSV: C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_inat_obs.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Join BSMB Obsservations and Convert to Arrays\n",
    "Create an array that shows all month/year combinations for the observation cycle with a 1 for observed and a 0 for not observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your GDB table (not a CSV)\n",
    "inat_table = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_inat_obs.csv\"\n",
    "doag_table = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_mndoag.csv\"\n",
    "\n",
    "# Step 1: Convert ArcGIS table to pandas DataFrame\n",
    "fields = ['UNIQUE_ID', 'Month', 'Year', 'Pres_Abs']  # adjust to match your actual field names\n",
    "data = [row for row in arcpy.da.SearchCursor(table_path, fields)]\n",
    "df = pd.DataFrame(data, columns=fields)\n",
    "\n",
    "# Drop or fill rows where 'Month' or 'Year' is NaN\n",
    "df_cleaned = df.dropna(subset=['Month', 'Year'])\n",
    "\n",
    "# Convert 'Month' and 'Year' to integers (safe now)\n",
    "df_cleaned['Month'] = df_cleaned['Month'].astype(int)\n",
    "df_cleaned['Year'] = df_cleaned['Year'].astype(int)\n",
    "\n",
    "# Create 'Month_Year' string\n",
    "df_cleaned['Month_Year'] = df_cleaned['Month'].astype(str).str.zfill(2) + \"/\" + df_cleaned['Year'].astype(str)\n",
    "\n",
    "# Step 2: Build full date range and CTU list\n",
    "date_range = pd.date_range(start=\"2018-10-01\", end=\"2025-07-01\", freq='MS')\n",
    "date_strs = [f\"{d.month:02d}/{d.year}\" for d in date_range]\n",
    "all_ctu_ids = pd.Series([row[0] for row in arcpy.da.SearchCursor(table_path, ['UNIQUE_ID'])]).unique()\n",
    "ctu_ids = all_ctu_ids\n",
    "\n",
    "# Step 3: Build complete CTU x month/year grid\n",
    "all_combos = pd.MultiIndex.from_product(\n",
    "    [ctu_ids, date_strs], names=['UNIQUE_ID', 'Month_Year']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# Step 4: Merge observations with full grid\n",
    "df_obs = df_cleaned[['UNIQUE_ID', 'Month_Year', 'Pres_Abs']].drop_duplicates()\n",
    "merged = pd.merge(all_combos, df_obs, on=['UNIQUE_ID', 'Month_Year'], how='left')\n",
    "merged['Pres_Abs'] = merged['Pres_Abs'].fillna(0)\n",
    "\n",
    "# Step 5: Pivot to make CTU x Time matrix\n",
    "pivot = merged.pivot(index='UNIQUE_ID', columns='Month_Year', values='Pres_Abs')\n",
    "\n",
    "# Step 6: Sort columns by date\n",
    "pivot = pivot[sorted(pivot.columns, key=lambda x: pd.to_datetime(x, format=\"%m/%Y\"))]\n",
    "\n",
    "# Step 7: Apply cumulative presence logic\n",
    "cumulative = pivot.apply(lambda row: row.cumsum().clip(upper=1), axis=1)\n",
    "\n",
    "# Optional: convert to numpy array\n",
    "presence_array = cumulative.to_numpy()\n",
    "\n",
    "# Preview\n",
    "print(\"Shape:\", presence_array.shape)\n",
    "print(cumulative.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (261, 82)\n",
      "Month_Year  10/2018  11/2018  12/2018  01/2019  02/2019  03/2019  04/2019  \\\n",
      "UNIQUE_ID                                                                   \n",
      "1               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "9               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "30              0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "32              0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "45              0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "Month_Year  05/2019  06/2019  07/2019  ...  10/2024  11/2024  12/2024  \\\n",
      "UNIQUE_ID                              ...                              \n",
      "1               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "9               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "30              0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "32              0.0      0.0      0.0  ...      1.0      1.0      1.0   \n",
      "45              0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "\n",
      "Month_Year  01/2025  02/2025  03/2025  04/2025  05/2025  06/2025  07/2025  \n",
      "UNIQUE_ID                                                                  \n",
      "1               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "9               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "30              0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "32              1.0      1.0      1.0      1.0      1.0      1.0      1.0  \n",
      "45              0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 82 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- File paths ---\n",
    "inat_table = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_inat_obs.csv\"\n",
    "doag_table = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_mndoag.csv\"\n",
    "\n",
    "# --- Load both datasets ---\n",
    "inat_df = pd.read_csv(inat_table)\n",
    "doag_df = pd.read_csv(doag_table)\n",
    "\n",
    "# --- Clean DOAG: drop nulls and keep earliest observation per UNIQUE_ID ---\n",
    "doag_df = doag_df.dropna(subset=['UNIQUE_ID', 'Month', 'Year'])\n",
    "doag_df['Month'] = doag_df['Month'].astype(int)\n",
    "doag_df['Year'] = doag_df['Year'].astype(int)\n",
    "\n",
    "# Create a proper date column to find the earliest observation\n",
    "doag_df['Date'] = pd.to_datetime(doag_df['Year'].astype(str) + '-' + doag_df['Month'].astype(str).str.zfill(2) + '-01')\n",
    "\n",
    "# Keep only the first record per UNIQUE_ID\n",
    "doag_earliest = doag_df.sort_values('Date').drop_duplicates(subset='UNIQUE_ID', keep='first')\n",
    "\n",
    "# Optional: Drop the 'Date' column after filtering\n",
    "doag_earliest = doag_earliest.drop(columns='Date')\n",
    "\n",
    "# --- Combine with iNat data ---\n",
    "inat_df = inat_df.dropna(subset=['UNIQUE_ID', 'Month', 'Year'])\n",
    "inat_df['Month'] = inat_df['Month'].astype(int)\n",
    "inat_df['Year'] = inat_df['Year'].astype(int)\n",
    "\n",
    "# Combine cleaned tables\n",
    "combined_df = pd.concat([inat_df, doag_earliest], ignore_index=True)\n",
    "\n",
    "# --- Create Month_Year and clean Pres_Abs ---\n",
    "combined_df['Month_Year'] = combined_df['Month'].astype(str).str.zfill(2) + \"/\" + combined_df['Year'].astype(str)\n",
    "combined_df['Pres_Abs'] = combined_df['Pres_Abs'].fillna(0).astype(int)\n",
    "\n",
    "# --- Full CTU x Month-Year grid ---\n",
    "date_range = pd.date_range(start=\"2018-10-01\", end=\"2025-07-01\", freq='MS')\n",
    "date_strs = [f\"{d.month:02d}/{d.year}\" for d in date_range]\n",
    "ctu_ids = combined_df['UNIQUE_ID'].unique()\n",
    "all_combos = pd.MultiIndex.from_product(\n",
    "    [ctu_ids, date_strs], names=['UNIQUE_ID', 'Month_Year']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# --- Merge with presence data ---\n",
    "obs_df = combined_df[['UNIQUE_ID', 'Month_Year', 'Pres_Abs']].drop_duplicates()\n",
    "merged = pd.merge(all_combos, obs_df, on=['UNIQUE_ID', 'Month_Year'], how='left')\n",
    "merged['Pres_Abs'] = merged['Pres_Abs'].fillna(0)\n",
    "\n",
    "# --- Pivot to matrix ---\n",
    "pivot = merged.pivot(index='UNIQUE_ID', columns='Month_Year', values='Pres_Abs')\n",
    "pivot = pivot[sorted(pivot.columns, key=lambda x: pd.to_datetime(x, format=\"%m/%Y\"))]\n",
    "\n",
    "# --- Fill from first observation onward ---\n",
    "def fill_from_first_observation(row):\n",
    "    obs_idx = row[row == 1].index\n",
    "    if len(obs_idx) == 0:\n",
    "        return row * 0\n",
    "    first_idx = row.index.get_loc(obs_idx[0])\n",
    "    row.iloc[first_idx:] = 1\n",
    "    row.iloc[:first_idx] = 0\n",
    "    return row\n",
    "\n",
    "cumulative = pivot.apply(fill_from_first_observation, axis=1)\n",
    "\n",
    "# --- Output path ---\n",
    "output_csv = os.path.join(os.path.dirname(inat_table), \"bmsb_combined_presence_matrix.csv\")\n",
    "cumulative.to_csv(output_csv)\n",
    "\n",
    "# --- Preview ---\n",
    "print(\"Matrix shape:\", cumulative.shape)\n",
    "print(cumulative.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matrix created with shape: (2743, 82)\n",
      "Month_Year  10/2018  11/2018  12/2018  01/2019  02/2019  03/2019  04/2019  \\\n",
      "UNIQUE_ID                                                                   \n",
      "0               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "1               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "2               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "3               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "4               0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "Month_Year  05/2019  06/2019  07/2019  ...  10/2024  11/2024  12/2024  \\\n",
      "UNIQUE_ID                              ...                              \n",
      "0               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "1               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "2               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "3               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "4               0.0      0.0      0.0  ...      0.0      0.0      0.0   \n",
      "\n",
      "Month_Year  01/2025  02/2025  03/2025  04/2025  05/2025  06/2025  07/2025  \n",
      "UNIQUE_ID                                                                  \n",
      "0               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "1               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "2               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "3               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "4               0.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 82 columns]\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- File paths ---\n",
    "inat_table = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_inat_obs.csv\"\n",
    "doag_table = r\"C:\\Users\\ethan\\Desktop\\GIS5572\\OUTPUT\\bmsb_mndoag.csv\"\n",
    "ctu_layer = \"ctus_with_id\"\n",
    "\n",
    "# --- Load observation datasets ---\n",
    "inat_df = pd.read_csv(inat_table)\n",
    "doag_df = pd.read_csv(doag_table)\n",
    "\n",
    "# --- Clean DOAG: keep earliest observation per UNIQUE_ID ---\n",
    "doag_df = doag_df.dropna(subset=['UNIQUE_ID', 'Month', 'Year'])\n",
    "doag_df['Month'] = doag_df['Month'].astype(int)\n",
    "doag_df['Year'] = doag_df['Year'].astype(int)\n",
    "doag_df['Date'] = pd.to_datetime(doag_df['Year'].astype(str) + '-' + doag_df['Month'].astype(str).str.zfill(2) + '-01')\n",
    "doag_earliest = doag_df.sort_values('Date').drop_duplicates(subset='UNIQUE_ID', keep='first').drop(columns='Date')\n",
    "\n",
    "# --- Clean iNat ---\n",
    "inat_df = inat_df.dropna(subset=['UNIQUE_ID', 'Month', 'Year'])\n",
    "inat_df['Month'] = inat_df['Month'].astype(int)\n",
    "inat_df['Year'] = inat_df['Year'].astype(int)\n",
    "\n",
    "# --- Combine both tables ---\n",
    "combined_df = pd.concat([inat_df, doag_earliest], ignore_index=True)\n",
    "combined_df['Month_Year'] = combined_df['Month'].astype(str).str.zfill(2) + \"/\" + combined_df['Year'].astype(str)\n",
    "combined_df['Pres_Abs'] = combined_df['Pres_Abs'].fillna(0).astype(int)\n",
    "\n",
    "# --- Get full list of CTUs from master layer ---\n",
    "ctu_ids = sorted({row[0] for row in arcpy.da.SearchCursor(ctu_layer, ['UNIQUE_ID'])})\n",
    "\n",
    "# --- Build full CTU x Month-Year grid ---\n",
    "date_range = pd.date_range(start=\"2018-10-01\", end=\"2025-07-01\", freq='MS')\n",
    "date_strs = [f\"{d.month:02d}/{d.year}\" for d in date_range]\n",
    "all_combos = pd.MultiIndex.from_product([ctu_ids, date_strs], names=['UNIQUE_ID', 'Month_Year']).to_frame(index=False)\n",
    "\n",
    "# --- Merge with observations ---\n",
    "obs_df = combined_df[['UNIQUE_ID', 'Month_Year', 'Pres_Abs']].drop_duplicates()\n",
    "merged = pd.merge(all_combos, obs_df, on=['UNIQUE_ID', 'Month_Year'], how='left')\n",
    "merged['Pres_Abs'] = merged['Pres_Abs'].fillna(0)\n",
    "\n",
    "# --- Pivot to matrix ---\n",
    "pivot = merged.pivot(index='UNIQUE_ID', columns='Month_Year', values='Pres_Abs')\n",
    "pivot = pivot[sorted(pivot.columns, key=lambda x: pd.to_datetime(x, format=\"%m/%Y\"))]\n",
    "\n",
    "# --- Apply \"1 from first observation forward\" logic ---\n",
    "def fill_from_first(row):\n",
    "    ones = row[row == 1]\n",
    "    if ones.empty:\n",
    "        return row * 0\n",
    "    first_index = row.index.get_loc(ones.index[0])\n",
    "    row.iloc[first_index:] = 1\n",
    "    row.iloc[:first_index] = 0\n",
    "    return row\n",
    "\n",
    "cumulative = pivot.apply(fill_from_first, axis=1)\n",
    "\n",
    "# --- Save cumulative presence matrix to CSV ---\n",
    "output_csv = os.path.join(os.path.dirname(inat_table), \"bmsb_combined_presence_matrix.csv\")\n",
    "cumulative.to_csv(output_csv)\n",
    "\n",
    "print(f\"Saved cumulative presence matrix to: {output_csv}\")\n",
    "\n",
    "# --- Preview ---\n",
    "print(\"✅ Matrix created with shape:\", cumulative.shape)\n",
    "print(cumulative.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
